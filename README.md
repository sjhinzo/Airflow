# ETL Apache Airflow - Automatic Workflow for Data Processing

## ğŸ“‹ Overview
This project demonstrates an automated ETL pipeline built with **Apache Airflow** to process, transform, and store data from multiple sources. The pipeline is containerized using **Docker**, making it easy to deploy and scalable for larger datasets. The workflow is fully managed via **Apache Airflow**, ensuring seamless execution and monitoring of each ETL stage.

## ğŸŒŸ Features
- **Automated ETL Workflow**: End-to-end automation from data download to transformation and export
- **Modular Task Design**: Tasks are reusable and easy to scale for larger projects
- **Error Handling**: Comprehensive logging and detailed monitoring for each ETL step
- **Flexible Output**: Data exported in both **CSV** and **JSON** formats for downstream use

## ğŸ—ï¸ Project Architecture
The ETL pipeline consists of the following stages:

1. **Download Dataset**: Automatically fetches `.tgz` data from a public URL
2. **Extract Data**: Extracts CSV and fixed-width files from the compressed archive
3. **Data Processing**:
   - Filters relevant fields from CSV and fixed-width files
   - Combines extracted data into a consolidated file
4. **Data Transformation**:
   - Standardizes fields (e.g., normalizes `Vehicle Type`)
5. **Export Data**:
   - Outputs to both **CSV** and **JSON** formats
6. **Validation**:
   - Verifies file integrity and checks for valid data rows

## ğŸ–¥ï¸ Technologies Used
- **Apache Airflow**: Workflow management and ETL task automation
- **Docker**: Environment containerization and deployment
- **Python**:
  - Libraries: `pandas`, `csv`, `tarfile`, `requests`, `json`
  - Data processing and transformation logic
- **PostgreSQL**: Airflow metadata backend database

## ğŸ“Š System Workflow
The DAG (Directed Acyclic Graph) structure in Airflow:

```
download_dataset --> extract_tgz --> [extract_data_from_csv, extract_data_from_fixed_width] 
      --> consolidate_data --> transform_data --> check_data_rows --> [export_to_json, loading_data]
      --> check_data_files
```

## ğŸ”§ Installation & Setup  & Demo Projects

### Prerequisites
- Docker and Docker Compose installed on your machine

### Setup Steps

1. **Clone the repository**:
   ```bash
   git clone https://github.com/sjhinzo/Airflow
   cd Airflow
   ```

3. **Start Docker**:
   ```bash
   docker-compose up -d
   ```
   ![image](https://github.com/user-attachments/assets/40a80fad-2c8a-442f-a72a-8ce69d9db3a1)
 **UI Docker**:
   ![image](https://github.com/user-attachments/assets/ae431347-3d23-4c9b-871b-aaad0d58f48b)

5. **Access Airflow**:
   - URL: http://localhost:8080
   - Username: airflow
   - Password: airflow
![image](https://github.com/user-attachments/assets/6f442225-dad0-4cac-8cc0-1523726bdfbe)
### **UI Airflow**:
![image](https://github.com/user-attachments/assets/de9cf037-effc-4706-92bb-862e6bf9b024)
## ğŸš€ Running the Workflow

### Step 1: Enable and Trigger DAG
- In Airflow UI, locate and enable the DAG `ETL_apache_airflow`
- Click "Trigger DAG" to start execution
![image](https://github.com/user-attachments/assets/f7934807-4065-41a4-ac15-d1473c714a33)

### Step 2: Monitor Workflow
- Use Graph View or Tree View to track task progress
- Task status indicators:
  - âœ… Green: Success
  - ğŸŸ¡ Yellow: Running
  - âŒ Red: Failed
![image](https://github.com/user-attachments/assets/6cf6fa56-787a-4592-b834-d585fc07170e)
### **GRAPH**:
![image](https://github.com/user-attachments/assets/e2a72e25-817f-4d29-b303-269b983b89ea)

### Step 3: Check Logs
- Click on individual tasks to view detailed execution logs
### **LOGS**:
![image](https://github.com/user-attachments/assets/66907f1e-a078-4878-b4ce-4924c5c0fba1)

### Step 4: Validate Output
Generated files:
- `transformed_data.csv`: Consolidated and normalized CSV file
- `transformed_data.json`: JSON format for API integration
![image](https://github.com/user-attachments/assets/e50f7aa3-dab0-40a4-bdd4-277a9bbe4410)

## ğŸ“‚ Project Structure
```
ğŸ“‚ Project
â”œâ”€â”€ ğŸ“‚ config                # Configuration files for Airflow
â”‚   â””â”€â”€ fernet_key.txt       # Fernet key for secure data encryption
â”œâ”€â”€ ğŸ“‚ dags                  # Contains all DAG definitions and related files
â”‚   â”œâ”€â”€ ğŸ“‚ staging           # Intermediate or staging data for DAGs
â”‚   â””â”€â”€ ETL.py               # Main DAG script for the ETL workflow
â”œâ”€â”€ ğŸ“‚ logs                  # Log files generated by Airflow
â”‚   â””â”€â”€ scheduler            # Logs specific to the Airflow scheduler
â”œâ”€â”€ ğŸ“‚ plugins               # Custom plugins for Airflow (if any)
â”œâ”€â”€ ğŸ“„ .env                  # Environment variables for the Docker setup
â”œâ”€â”€ ğŸ“„ .gitattributes        # Git configuration for managing files
â”œâ”€â”€ ğŸ“„ docker-compose.yaml   # Docker Compose configuration to run Airflow services

```

## ğŸŒŸ Key Benefits
- Fully automated data pipeline with minimal manual intervention
- Scalable architecture supporting additional data sources/transformations
- Clean and reusable codebase for future projects

## ğŸ“§ Contact
For questions or issues, please:
- Open an issue in this repository
- Contact: buitiensang191@gmail.com
- Linkedin: https://www.linkedin.com/in/sjhinzo/
